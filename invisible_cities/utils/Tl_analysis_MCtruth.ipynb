{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook contains the analysis of the MC information truth of Tl source "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import tables as tb\n",
    "import numpy  as np\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.mlab as mlab\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from   invisible_cities.database               import load_db\n",
    "\n",
    "import invisible_cities.reco.paolina_functions as plf\n",
    "from   invisible_cities.reco.dst_functions     import load_xy_corrections\n",
    "\n",
    "from   invisible_cities.io.dst_io              import load_dst\n",
    "from   invisible_cities.io.hits_io             import load_hits\n",
    "from   invisible_cities.io.hits_io             import load_hits_skipping_NN\n",
    "from invisible_cities.io.mchits_io             import load_mchits\n",
    "\n",
    "from   invisible_cities.types.ic_types         import xy\n",
    "from   invisible_cities.types.ic_types         import NN\n",
    "\n",
    "from   invisible_cities.evm.event_model        import Cluster, Hit\n",
    "\n",
    "import invisible_cities.core.fit_functions     as fitf\n",
    "\n",
    "from   invisible_cities.icaro.hst_functions    import hist\n",
    "from   invisible_cities.icaro.hst_functions    import gausstext\n",
    "\n",
    "from   invisible_cities.database               import load_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def merge_NN_hits(hits_all,hits_nonNN):\n",
    "\n",
    "    # Iterate through the nonNN dictionary and update the energies including the NN hits from the \"all\" dictionary.\n",
    "    for (evt,hc) in hits_nonNN.items():\n",
    "        # Get the corresponding collection of all hits.\n",
    "        hc_all = hits_all[evt]            \n",
    "        # Add energy from all NN hits to hits in closest slice.\n",
    "        for h1 in hc_all.hits:\n",
    "            if(h1.Q == NN):\n",
    "                # Find the hits to which the energy will be added.\n",
    "                zdist_min = -1\n",
    "                h_add = []\n",
    "                for h2 in hc.hits:\n",
    "                    zdist = np.abs(h1.Z - h2.Z)\n",
    "                    if(zdist_min < 0 or zdist < zdist_min):\n",
    "                        zdist_min = zdist\n",
    "                        h_add = []\n",
    "                        h_add.append(h2)\n",
    "                    elif(zdist == zdist_min):\n",
    "                        h_add.append(h2)\n",
    "\n",
    "                # Add the energy.\n",
    "                hadd_etot = sum([ha.E for ha in h_add])\n",
    "                for ha in h_add:\n",
    "                    ha.energy += h1.E*(ha.E/hadd_etot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DetGeo = load_db.DetectorGeo()\n",
    "fwhmQbb = 0.006\n",
    "vox_size = np.array([15,15,15],dtype=np.int16)    # voxel size\n",
    "blob_radius = 21.   # blob radius in mm\n",
    "file_base_name = 'tl_pmaps_7bar_NEXT_v1_00_05_v0.9.2_20171011_krmc_tl'\n",
    "directory_name = '/home/Tl_v0.9.2'\n",
    "start = 0 ## analyzes first file 0\n",
    "numb = 5 ## number of files to be analyzed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Containers for variables to be saved to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "evt_energies = []\n",
    "trk_energies = []\n",
    "number_of_tracks = []\n",
    "number_of_voxels = []\n",
    "number_of_voxels_main = []\n",
    "length_main = []\n",
    "energy_main = []\n",
    "l_eblob1 = []; l_eblob2 = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop on the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for n in range(start,start+numb):\n",
    "    lhits = []\n",
    "    hits_file = '{0}/{1}_hits.{2}.h5'.format(directory_name, file_base_name, n)\n",
    "    print('Analyzing {0}'.format(hits_file))\n",
    "\n",
    "    hit_dict = load_mchits(hits_file)\n",
    "\n",
    "    # Correct the energy of the hits, one by one\n",
    "    hitc_cevt = {}\n",
    "    for evt, hits in hit_dict.items():\n",
    "        \n",
    "        mc_evt_energy = sum([h.E for h in hits])\n",
    "        sigmaE = fwhmQbb * np.sqrt(2.458)*np.sqrt(mc_evt_energy)/2.35\n",
    "        smeared_evt_energy = mc_evt_energy + random.gauss(0, 1) * sigmaE\n",
    "        conv_factor = smeared_evt_energy/mc_evt_energy\n",
    "\n",
    "        hc_smeared = []\n",
    "        for hit in hits:\n",
    "        # Check that only the hits of the ACTIVE volume are considered\n",
    "            if hit.Z < DetGeo.ZMAX[0] and hit.X < DetGeo.XMAX[0] and hit.X > DetGeo.XMIN[0]:\n",
    "                hsmeared = Hit(0,Cluster(0, xy(hit.X,hit.Y), xy(0,0), 0), hit.Z, hit.E*1000.*conv_factor)\n",
    "                hc_smeared.append(hsmeared)\n",
    "        hitc_cevt[evt] = hc_smeared\n",
    "\n",
    "    # Build tracks and find blobs\n",
    "    for nevt, hitc in hitc_cevt.items():\n",
    "\n",
    "        tot_e = sum([hh.E for hh in hitc])\n",
    "        evt_energies.append(tot_e)\n",
    "                       \n",
    "        voxels = plf.voxelize_hits(hitc,vox_size)\n",
    "        trks = plf.make_track_graphs(voxels,vox_size)\n",
    "        number_of_tracks.append(len(trks))\n",
    "    \n",
    "        energy_tr = []\n",
    "        for t in trks:\n",
    "            if(len(t.nodes()) < 1):\n",
    "                etrk = 0\n",
    "            else:\n",
    "                etrk = sum([vox.E for vox in t.nodes()])\n",
    "                energy_tr.append(etrk)\n",
    "                trk_energies.append(etrk)\n",
    "                number_of_voxels.append(len(t.nodes()))\n",
    "   \n",
    "                if len(trks) == 0:\n",
    "                    continue\n",
    "        \n",
    "                itmax = np.argmax(energy_tr)\n",
    "    \n",
    "                number_of_voxels_main.append(len(trks[itmax].nodes()))\n",
    "                e_main = sum([vox.E for vox in trks[itmax].nodes()])\n",
    "                energy_main.append(e_main)\n",
    "\n",
    "                if e_main < 1576. or e_main > 1607.:\n",
    "                    continue\n",
    "\n",
    "                Eblob1, Eblob2 = plf.blob_energies(trks[itmax],blob_radius)\n",
    "                               \n",
    "                # Ensure blob1 always has higher energy\n",
    "                if(Eblob2 > Eblob1):\n",
    "                    eswap = Eblob1\n",
    "                    Eblob1 = Eblob2\n",
    "                    Eblob2 = eswap\n",
    "              \n",
    "                l_eblob1.append(Eblob1)\n",
    "                l_eblob2.append(Eblob2)\n",
    "                length_main.append(plf.length(trks[itmax]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a_eblob1 = np.array(l_eblob1)\n",
    "a_eblob2 = np.array(l_eblob2)\n",
    "\n",
    "a_evt_energies = np.array(evt_energies)\n",
    "a_trk_energies = np.array(trk_energies)\n",
    "a_number_of_tracks = np.array(number_of_tracks)\n",
    "a_number_of_voxels = np.array(number_of_voxels)\n",
    "a_number_of_voxels_main = np.array(number_of_voxels_main)\n",
    "a_length_main = np.array(length_main)\n",
    "a_energy_main = np.array(energy_main)\n",
    "\n",
    "evt_file = '{0}/analysis_{1}_truth'.format(directory_name, file_name)\n",
    "np.savez(evt_file, a_eblob1=a_eblob1, a_eblob2=a_eblob2, a_evt_energies=a_evt_energies, a_trk_energies=a_trk_energies, a_number_of_tracks=a_number_of_tracks, a_number_of_voxels=a_number_of_voxels, a_number_of_voxels_main=a_number_of_voxels_main, a_length_main=a_length_main, a_energy_main=a_energy_main) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
